{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fae918f-fdbb-4628-8274-226fe3d511ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "    datasets==2.12.0 \\\n",
    "    apache_beam \\\n",
    "    mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716289ad-052c-421d-86fb-6e9782d35986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314190"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('meditations copy 2.text', \"r\") as f: \n",
    "    data = f.read()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "331e4eb9-2f95-4c72-8298-4e6149f32628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.0.162\n",
      "  Using cached langchain-0.0.162-py3-none-any.whl (770 kB)\n",
      "Collecting openai==0.27.7\n",
      "  Using cached openai-0.27.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken==0.4.0\n",
      "  Using cached tiktoken-0.4.0-cp310-cp310-macosx_10_9_x86_64.whl (797 kB)\n",
      "Collecting pinecone-client==2.2.2 (from pinecone-client[grpc]==2.2.2)\n",
      "  Using cached pinecone_client-2.2.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from langchain==0.0.162) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.0.162)\n",
      "  Using cached SQLAlchemy-2.0.25-cp310-cp310-macosx_10_9_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.0.162)\n",
      "  Using cached aiohttp-3.9.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain==0.0.162)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.162)\n",
      "  Using cached dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.162)\n",
      "  Using cached numexpr-2.8.8-cp310-cp310-macosx_10_9_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from langchain==0.0.162) (1.26.3)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.162)\n",
      "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Collecting pydantic<2,>=1 (from langchain==0.0.162)\n",
      "  Using cached pydantic-1.10.13-cp310-cp310-macosx_10_9_x86_64.whl.metadata (149 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from langchain==0.0.162) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.162)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from langchain==0.0.162) (4.66.1)\n",
      "Collecting regex>=2022.1.18 (from tiktoken==0.4.0)\n",
      "  Using cached regex-2023.12.25-cp310-cp310-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from pinecone-client==2.2.2->pinecone-client[grpc]==2.2.2) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from pinecone-client==2.2.2->pinecone-client[grpc]==2.2.2) (4.9.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from pinecone-client==2.2.2->pinecone-client[grpc]==2.2.2) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from pinecone-client==2.2.2->pinecone-client[grpc]==2.2.2) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from pinecone-client==2.2.2->pinecone-client[grpc]==2.2.2) (2.1.0)\n",
      "Collecting grpcio>=1.44.0 (from pinecone-client[grpc]==2.2.2)\n",
      "  Using cached grpcio-1.60.0.tar.gz (24.8 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting grpc-gateway-protoc-gen-openapiv2==0.1.0 (from pinecone-client[grpc]==2.2.2)\n",
      "  Using cached grpc_gateway_protoc_gen_openapiv2-0.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting googleapis-common-protos>=1.53.0 (from pinecone-client[grpc]==2.2.2)\n",
      "  Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting lz4>=3.1.3 (from pinecone-client[grpc]==2.2.2)\n",
      "  Using cached lz4-4.3.3-cp310-cp310-macosx_10_9_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting protobuf~=3.19.5 (from pinecone-client[grpc]==2.2.2)\n",
      "  Using cached protobuf-3.19.6-py2.py3-none-any.whl (162 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.162) (23.2.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.162)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-macosx_10_9_x86_64.whl (29 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.162)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-macosx_10_9_x86_64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.162)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.162)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.162)\n",
      "  Using cached marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.162)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone-client==2.2.2->pinecone-client[grpc]==2.2.2) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.162) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.162) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.162) (2023.11.17)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.0.162)\n",
      "  Using cached greenlet-3.0.3.tar.gz (182 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /Users/arshad_221b/anaconda3/envs/pdfreader/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.162) (23.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.162)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached openai-0.27.7-py3-none-any.whl (71 kB)\n",
      "Using cached pinecone_client-2.2.2-py3-none-any.whl (179 kB)\n",
      "Using cached aiohttp-3.9.1-cp310-cp310-macosx_10_9_x86_64.whl (397 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "Using cached lz4-4.3.3-cp310-cp310-macosx_10_9_x86_64.whl (254 kB)\n",
      "Using cached numexpr-2.8.8-cp310-cp310-macosx_10_9_x86_64.whl (102 kB)\n",
      "Using cached pydantic-1.10.13-cp310-cp310-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "Using cached regex-2023.12.25-cp310-cp310-macosx_10_9_x86_64.whl (296 kB)\n",
      "Using cached SQLAlchemy-2.0.25-cp310-cp310-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-macosx_10_9_x86_64.whl (53 kB)\n",
      "Using cached marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-macosx_10_9_x86_64.whl (81 kB)\n",
      "Building wheels for collected packages: grpcio, greenlet\n",
      "  Building wheel for grpcio (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for grpcio: filename=grpcio-1.60.0-cp310-cp310-macosx_10_10_x86_64.whl size=4335144 sha256=f710b0fbf1a70b60fca9336b0b4947c376d74f80adde6054cdaec47a8ba2d1d2\n",
      "  Stored in directory: /Users/arshad_221b/Library/Caches/pip/wheels/95/99/28/4e3391b171168454a02d5e33906ff7e234e872c237df9d652d\n",
      "  Building wheel for greenlet (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for greenlet: filename=greenlet-3.0.3-cp310-cp310-macosx_10_9_x86_64.whl size=214019 sha256=8e07d4c2c0aa20db97fa515497e245f969616898803665d99f4c3c6998f799f2\n",
      "  Stored in directory: /Users/arshad_221b/Library/Caches/pip/wheels/3a/96/ec/7725e96479eb6d95655576826b2ccafd788805f2a89478331c\n",
      "Successfully built grpcio greenlet\n",
      "Installing collected packages: tenacity, regex, pydantic, protobuf, numexpr, mypy-extensions, multidict, marshmallow, lz4, grpcio, greenlet, frozenlist, async-timeout, yarl, typing-inspect, tiktoken, SQLAlchemy, pinecone-client, openapi-schema-pydantic, googleapis-common-protos, aiosignal, grpc-gateway-protoc-gen-openapiv2, dataclasses-json, aiohttp, openai, langchain\n",
      "  Attempting uninstall: pinecone-client\n",
      "    Found existing installation: pinecone-client 2.2.4\n",
      "    Uninstalling pinecone-client-2.2.4:\n",
      "      Successfully uninstalled pinecone-client-2.2.4\n",
      "Successfully installed SQLAlchemy-2.0.25 aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 dataclasses-json-0.5.14 frozenlist-1.4.1 googleapis-common-protos-1.62.0 greenlet-3.0.3 grpc-gateway-protoc-gen-openapiv2-0.1.0 grpcio-1.60.0 langchain-0.0.162 lz4-4.3.3 marshmallow-3.20.2 multidict-6.0.4 mypy-extensions-1.0.0 numexpr-2.8.8 openai-0.27.7 openapi-schema-pydantic-1.2.4 pinecone-client-2.2.2 protobuf-3.19.6 pydantic-1.10.13 regex-2023.12.25 tenacity-8.2.3 tiktoken-0.4.0 typing-inspect-0.9.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.0.162 \\\n",
    "  openai==0.27.7 \\\n",
    "  tiktoken==0.4.0 \\\n",
    "  \"pinecone-client[grpc]\"==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8e623471-dadd-46d6-b2e8-9731e94434c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def split_into_sentence_chunks(text, max_chunk_length):\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    current_chunk = \"\"\n",
    "    chunks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_length:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "file_contents = data\n",
    "max_chunk_length = 200  # Choose the maximum length for each chunk\n",
    "sentence_chunks = split_into_sentence_chunks(file_contents, max_chunk_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d295bc9f-032e-4cd9-a1b5-ee047fc91901",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_chunks = [sentence for sentence in sentence_chunks if len(sentence) >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "92393b94-945d-4795-9c78-a3c85a4b5b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Roman Emperor His First Book oncerning himself Wherein Antoninus recordeth, What and of whom, whether Parents, Friends, or asters; by their good examples, or good advice and counsel, he had learned: ivided into Numbers or Sections.'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "49abe09c-efc6-4f71-bed2-438b8378fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get openai api key from platform.openai.com\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "27cdac72-32fb-4663-a863-3c8e9d350357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b1661bd2-d5b2-4329-b481-5796f2ba489c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1558, 1536)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = embed.embed_documents(sentence_chunks)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "245c4671-8796-4315-943f-d33a95e8738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'pdfsearch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a6da4085-4a9c-4f87-99c2-b1604e57291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# find API key in console at app.pinecone.io\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n",
    "# find ENV (cloud region) next to API key in console\n",
    "PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT') or 'PINECONE_ENVIRONMENT'\n",
    "\n",
    "pinecone.init(\n",
    "    api_key='PINECONE_API_KEY',\n",
    "    environment='PINECONE_ENVIRONMENT'\n",
    ")\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=len(res[0])  # 1536 dim of text-embedding-ada-002\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3e3a9826-5805-4a50-85e5-1874f3bef8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 0}},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.GRPCIndex(index_name)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef7572-6470-432d-a8be-d1c6b001f65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "88074210-192a-488c-960f-2e735ed49348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████▋                   | 4/7 [00:39<00:28,  9.40s/it]Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-DALsQsfLBMuaNvlMG6TH8Z7n on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      " 71%|████████████████████████████████▏            | 5/7 [00:52<00:21, 10.78s/it]Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-DALsQsfLBMuaNvlMG6TH8Z7n on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-DALsQsfLBMuaNvlMG6TH8Z7n on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      " 86%|██████████████████████████████████████▌      | 6/7 [01:12<00:13, 13.82s/it]Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-DALsQsfLBMuaNvlMG6TH8Z7n on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-DALsQsfLBMuaNvlMG6TH8Z7n on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "100%|█████████████████████████████████████████████| 7/7 [01:29<00:00, 12.77s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "import time\n",
    "\n",
    "batch_size = 250\n",
    "for i in tqdm(range(0, len(sentence_chunks), batch_size)):\n",
    "    \n",
    "    i_min = min(i+batch_size, len(sentence_chunks))\n",
    "    batch = sentence_chunks[i: i_min]\n",
    "    meta_data = [{\"titile\" : 'Marcus Aurelius Meditations', \n",
    "              \"context\": row}\n",
    "                for row in batch]\n",
    "    ids = [str(uuid4()) for _ in range(len(batch))]\n",
    "    # Encode the text to obtain its vector representation\n",
    "    embeds = embed.embed_documents(batch)\n",
    "    \n",
    "    # Upsert the vector and text into the Pinecone index\n",
    "    index.upsert(vectors=zip(ids, embeds, meta_data))\n",
    "    time.sleep(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e2cb0189-020b-4023-9926-989fd0016945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.01558,\n",
       " 'namespaces': {'': {'vector_count': 1558}},\n",
       " 'total_vector_count': 1558}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "82b0aa96-eb3b-4e64-a734-a411a61dc962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"context\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9dca5e8d-cf07-49e2-84c4-b1b92898eac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='then thou shalt separate from thyself, that is from thy mind, whatsoever other men either do or say, or whatsoever thou thyself hast heretofore either done or said; and all trouble- some thoughts concerning the future, and whatsoever, (as either belonging to thy body or life): is without the jurisdiction of thine own will, and whatsoever in the ordi- nary course of human chances and accidents doth happen unto thee; so that thy mind (keeping herself loose and free from all outward coincidental entanglements; always in a readiness to depart): shall live by herself, and to herself, doing that which is just, accepting whatsoever doth happen, and speaking the truth always; if, say, thou shalt separate from thy mind, whatsoever by sympathy might adhere unto it, and all time both past and future, and shalt make thyself in all points and respects, like unto Empedocles his allegorical sphere, “all round and circular,” &., and shalt think of no longer life than that which is now present: then shalt thou be truly able to pass the remainder of thy days without troubles and distractions; nobly and gen- erously disposed, and in good favour and correspondency, with that spirit which is within thee.', metadata={'titile': 'Marcus Aurelius Meditations'}),\n",
       " Document(page_content='And philosophy doth consist in this, for a man to preserve that spirit which is within him, from all manner of contumelies and injuries, and above all pains or pleasures; never to do anything either rashly, or feignedly, or hypocritically: wholly to depend from himself and his own proper actions: all things that happen unto him to embrace contentedly, as coming from Him from whom he himself also came; and above all things, with all meekness and a calm cheerfulness, to expect death, as be- ing nothing else but the resolution of those elements, of which every creature is com- posed.', metadata={'titile': 'Marcus Aurelius Meditations'}),\n",
       " Document(page_content='Remember, that that which sets a man at work, and hath power over the affections to draw them either one way, or the other way, is not any external thing properly, but that which is hidden within every man’ dogmata, and opinions: That, that is rhetoric; that is life; that (to speak true) is man himself.', metadata={'titile': 'Marcus Aurelius Meditations'})]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Explore the dichotomy between the inner self and external circumstances as presented by Marcus Aurelius\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "378bde5e-6228-4ae4-aaa0-d1a11e613638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory \\\n",
    "import ConversationBufferWindowMemory\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# OpenAI LLM\n",
    "llm = ChatOpenAI(openai_api_key = OPENAI_API_KEY,\n",
    "                model_name = 'gpt-3.5-turbo',\n",
    "                temperature = 0.0)\n",
    "\n",
    "# conversational memory\n",
    "conv_mem = ConversationBufferWindowMemory(\n",
    "    memory_key = 'chat_history',\n",
    "    k = 5,\n",
    "    return_messages =True)\n",
    "\n",
    "# retrieval qa\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d6b60738-011f-49b3-b200-2e284b540688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the passage, Marcus Aurelius emphasizes the importance of separating oneself from external circumstances and focusing on the inner self. He advises to detach from the thoughts and actions of others, as well as from past and future events that are beyond one's control. By doing so, one can live with a free and clear mind, accepting whatever happens and speaking the truth.\\n\\nAurelius suggests that true philosophy lies in preserving one's inner spirit, regardless of insults, injuries, or even pleasures and pains. It involves acting sincerely and authentically, relying on one's own actions, and embracing everything that comes as part of a greater plan. He encourages a calm acceptance of death, viewing it as the natural dissolution of the elements that make up every living being.\\n\\nThe passage also highlights the power of one's own thoughts and opinions in shaping their actions and emotions. Aurelius states that it is not external circumstances that influence a person, but rather their internal beliefs and judgments. He equates this influence to rhetoric, life, and ultimately, the essence of being human.\\n\\nFurthermore, Aurelius mentions the ability to want and enjoy things in moderation as a sign of strength and self-control. He refers to Socrates as an example of someone who knew how to navigate both the absence and abundance of certain things without weakness or excess. This moderation and sobriety, according to Aurelius, are characteristic of a person with a perfect and invincible soul.\\n\\nOverall, Marcus Aurelius emphasizes the importance of focusing on the inner self, detaching from external circumstances, and maintaining a calm and balanced mindset in order to live a virtuous and fulfilling life.\""
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Explore the dichotomy between the inner self and external circumstances as presented by Marcus Aurelius\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679873e-7026-480c-92df-a4f47ced48e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
